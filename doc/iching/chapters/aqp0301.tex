\newpage
\maketitle
\begin{center}
\Large \textbf{第301章 MAML算法} \quad 
\end{center}
\begin{abstract}
在本章中，我们将讲解MAML算法的基本原理，并且以Omniglot数据集为例，讲解一个5-way 1-shot的算法实现，并且复现论文中的结果。
\end{abstract}
\section{MAML算法}
在元学习中，最著名的算法当属MAML算法，在本章中，我们将讲解MAML算法数学原理和PyTorch实现技术。
\subsection{Omniglot数据集}
Omniglot数据集的目录结构如下所示：
\begin{figure}[H]
	\caption{Omniglot数据集目录结构}
	\label{f000116}
	\centering
	\includegraphics[width=15cm]{images/f000116}
\end{figure}
数据集类代码如下所示：
\lstset{language=PYTHON, caption={Omniglot数据集类}, label={c0301-omniglot-ds}}
\begin{lstlisting}
class OmniglotDs(Dataset):
    def __init__(self, data_dir, k_way, q_query):
        self.file_list = [f for f in glob.glob(data_dir + 
                    '**/character*', recursive=True)]
        self.transform = transforms.Compose([transforms.ToTensor()])
        self.n = k_way + q_query

    def __getitem__(self, idx):
        sample = np.arange(20)
        np.random.shuffle(sample)
        img_path = self.file_list[idx]
        img_list = [f for f in glob.glob(img_path + "**/*.png", 
                    recursive=True)]
        img_list.sort()
        imgs = [self.transform(Image.open(img_file))
                     for img_file in img_list]
        imgs = torch.stack(imgs)[sample[:self.n]]
        return imgs

    def __len__(self):
        return len(self.file_list)   
\end{lstlisting}
代码解读如下所示：
\begin{itemize}
\item 第3、4行：取出图片文件最后一层文件目录的列表，如下所示：
\lstset{language=BASH, caption={self.file\_list内容}, label={c0301-omniglot-ds-file-list}}
\begin{lstlisting}
./data/Omniglot/images_background\Alphabet_of_the_Magi.0\character01
./data/Omniglot/images_background\Alphabet_of_the_Magi.0\character02
./data/Omniglot/images_background\Alphabet_of_the_Magi.0\character03
./data/Omniglot/images_background\Alphabet_of_the_Magi.0\character04
./data/Omniglot/images_background\Alphabet_of_the_Magi.0\character05
./data/Omniglot/images_background\Alphabet_of_the_Magi.0\character06
./data/Omniglot/images_background\Alphabet_of_the_Magi.0\character07
./data/Omniglot/images_background\Alphabet_of_the_Magi.0\character08
\end{lstlisting}
\item 第6行：每个批次中Support Set的大小为k\_way，Query Set的大小为q\_query，该批次的总大小为self.n；
\item 第11行：取到某一个包含图片文件的目录；
\item 第12$\sim$14行取出该目录下所有图片文件列表，每个目录下有20个文件；
\item 第15、16行：类型为list[tensor]，将目录下每个图片内容读出来转为tensor，形状为$[1, 28, 28]$，即黑白图片（通道数），分辨率为$28 \times 28$，20个图片文件形成的tensor组成一个list；
\item 第17行：因为在第9行，生成[0,1,...,19]的列表，然后将其随机进行排序，运行结果如下所示：
\lstset{language=BASH, caption={运行原理}, label={c0301-omniglot-ds-get-item}}
\begin{lstlisting}
sample: [ 7  0 18 16 15 17  9 12 11  1  5 14  4  6  8  3  2 10 19 13];
sample[:self.n]: [7 0];
[sample[:self.n]]: [array([7, 0])];
imgs: torch.Size([2, 1, 28, 28]);
\end{lstlisting}
由此可见其形成的是一个训练中用到的迷你批次。
\end{itemize}
下面我们来看模型类OgmlModel，如下所示：
\lstset{language=PYTHON, caption={OgmlModel类}, label={c0301-ogml-model}}
\begin{lstlisting}
def ConvBlock(in_ch, out_ch):
    return nn.Sequential(nn.Conv2d(in_ch, out_ch, 3, padding = 1),
                        nn.BatchNorm2d(out_ch),
                        nn.ReLU(),
                        nn.MaxPool2d(kernel_size = 2, stride = 2))

def ConvBlockFunction(x, w, b, w_bn, b_bn):
    x = F.conv2d(x, w, b, padding = 1)
    x = F.batch_norm(x, running_mean = None, running_var = None, 
                weight = w_bn, bias = b_bn, training = True)
    x = F.relu(x)
    x = F.max_pool2d(x, kernel_size = 2, stride = 2)
    return x

class OgmlModel(nn.Module):
    def __init__(self, in_ch, n_way):
        super(OgmlModel, self).__init__()
        self.conv1 = ConvBlock(in_ch, 64)
        self.conv2 = ConvBlock(64, 64)
        self.conv3 = ConvBlock(64, 64)
        self.conv4 = ConvBlock(64, 64)
        self.logits = nn.Linear(64, n_way)
    
    def forward(self, x):
        x = self.conv1(x)
        x = self.conv2(x)
        x = self.conv3(x)
        x = self.conv4(x)
        x = nn.Flatten(x)
        x = self.logits(x)
        return x

    def functional_forward(self, x, params):
        '''
        Arguments:
        x: input images [batch, 1, 28, 28]
        params: 模型的參數，也就是 convolution 的 weight 跟 bias，
                以及 batchnormalization 的  weight 跟 bias
                這是一個 OrderedDict
        '''
        for block in [1, 2, 3, 4]:
            x = ConvBlockFunction(x, params[f'conv{block}.0.weight'], 
                        params[f'conv{block}.0.bias'],                                 
                        params.get(f'conv{block}.1.weight'), 
                        params.get(f'conv{block}.1.bias'))
        x = x.view(x.shape[0], -1)
        x = F.linear(x, params['logits.weight'] , params['logits.bias'])
        return x
\end{lstlisting}
OgmlModel有两种工作模式，一种是正常的forward模式，由构造函数、ConvBolock函数和forward方法定义，另一种是functional\_forward模式，由ConvBlockFunction和functional\_forward方法组成，在程序中我们使用的是functional\_forward方法，因此我们只讲解这种工作模式。
\begin{itemize}
\item 第18行：生成第一个卷积层，调用ConvBlock方法：
	\begin{itemize}
	\item 第2行：调用torch.nn.Conv2d函数，输入信号通道数in\_ch为1，输出信号通道数out\_ch为类别数，这里为5，卷积核为$3 \times 3$，边缘零填充为1，即经过卷积后输入信号维度不变；
	\item 第3行：调用torch.nn.BatchNorm2d函数，进行批归一化，其输入信号格式为[N, C, H, W]，其中N为批次中序号，C为通道数，H为高，W为宽，归一化公式为：
\begin{equation}
\tilde{x} = \frac{x-E(x)}{ \sqrt{ Var(x) + \epsilon } } * \gamma + \beta
\label{e0301-batch-norm-2d-formula}
\end{equation}
	\item 第4行：经过ReLU函数；
	\item 第5行：调用MaxPool2d函数，每$2 \times 2$中取最大值，步长为2，即将图像尺寸缩小为原来的$\frac{1}{4}$；
	\end{itemize}
\item 第19行：生成第2个卷积层块；
\item 第20行：生成第3个卷积层块；
\item 第21行：生成第4个卷积层块；
\item 第22行：生成最后的输出层，输出层为类别数，经过第1个卷积层块尺寸变为$14 \times 14$，第2个卷积层块后为$7 \times 7$，经过第3个卷积层块为块为$3 \times 3$，经过第4个卷积块为$1 \times 1$，所以其就只有64个神经元；
\end{itemize}
下面我们来看训练过程，首先来看训练入口方法：
\lstset{language=PYTHON, caption={训练入口类}, label={c0301-train-main}}
\begin{lstlisting}
def train(self):
        n_way = 5
        k_shot = 1
        q_query = 1
        inner_train_steps = 1
        inner_lr = 0.4
        meta_lr = 0.001
        meta_batch_size = 32
        max_epoch = 4 #40
        eval_batches = 20
        train_data_path = './data/Omniglot/images_background/'
        dataset = OmniglotDs(train_data_path, k_shot, q_query)
        train_set, val_set = torch.utils.data.random_split(
                    dataset, [3200,656])
        train_loader = DataLoader(train_set,
                                batch_size = n_way,
                                num_workers = 8,
                                shuffle = True,
                                drop_last = True)
        val_loader = DataLoader(val_set,
                                batch_size = n_way,
                                num_workers = 8,
                                shuffle = True,
                                drop_last = True)
        train_iter = iter(train_loader)
        val_iter = iter(val_loader)
        #
        meta_model = OgmlModel(1, n_way).to(self.device)
        optimizer = torch.optim.Adam(meta_model.parameters(), lr = meta_lr)
        loss_fn = nn.CrossEntropyLoss().to(self.device)
        for epoch in range(max_epoch):
            print("Epoch %d" %(epoch))
            train_meta_loss = []
            train_acc = []
            for step in tqdm(range(len(train_loader) // 
                        (meta_batch_size))): 
                x, train_iter = self.get_meta_batch(
                    meta_batch_size, k_shot, q_query, 
                    train_loader, train_iter
                )
                print('x: {0} - {1};'.format(type(x), x.shape))
                sys.exit(0)
                meta_loss, acc = self.train_batch(
                    meta_model, optimizer, x, n_way, 
                    k_shot, q_query, loss_fn
                )
                train_meta_loss.append(meta_loss.item())
                train_acc.append(acc)
            print("  Loss    : ", np.mean(train_meta_loss))
            print("  Accuracy: ", np.mean(train_acc))
            val_acc = []
            for eval_step in tqdm(range(len(val_loader) // 
                        (eval_batches))):
                x, val_iter = self.get_meta_batch(
                    eval_batches, k_shot, q_query, 
                    val_loader, val_iter
                )
                _, acc = self.train_batch(
                    meta_model, optimizer, x, n_way, 
                    k_shot, q_query, loss_fn, 
                    inner_train_steps = 3, train = False
                )
                val_acc.append(acc)
            print("  Validation accuracy: ", np.mean(val_acc))
        print('train is OK!')
        torch.save(meta_model.state_dict(), self.chpt_file)
\end{lstlisting}
代码解读如下所示：
\begin{itemize}
\item 第2$\sim$10行：定义所需变量，n\_way代表类别数，k\_shot是迷你批次中support set的样本数，q\_query是迷你批次中query set的样本数；
\item 第11$\sim$26行：定义训练数据集和验证数据集；
\item 第28行：创建模型，创建卷积层和最后的全连接层；
\item 第29行：定义优化器；
\item 第30行：定义代价函数为交叉熵函数；
\item 第31$\sim$61行：训练指定epochs遍：
\item 第33行：train\_meta\_loss；
\item 第34行：train\_acc；
\item 第35$\sim$48行：循环处理训练数据集每个迷你批次：
\item 第37$\sim$40行：取出一个迷你批次，其形状为[32, 10, 1, 28, 28]，迷你批次大小为32，共有5个类别，每个类别有support set（这里是1个样本），query set（这里是1个样本），所以10=n\_way * (support\_set + query\_set)；
\item 第43$\sim$46行：训练一个迷你批次，包括一次前向传播和误差反向传播过程，具体实现将在后面详述，计算出代价函数值和精度；
\item 第47行：求迷你批次中代价函数之和；
\item 第48行：求迷你批次中精度之和；
\item 第49行：显示迷你批次的平均代价函数值；
\item 第50行：显示迷你批次的平均精度值；
\item 第52、53行：循环处理验证数据集每个迷你批次：
\item 第54$\sim$57行：取出一个迷你批次；
\item 第58$\sim$62行：求出每个迷你批次上的精度，这里只有前向传播过程，没有误差反向传播；
\item 第63行：形成迷你批次精度列表；
\item 第64行：打印批你批次平均精度；
\item 第66行：训练完成后保存网络状态；
\end{itemize}
下面我们来看迷你批次的训练过程，如下所示：
\lstset{language=PYTHON, caption={迷你批次训练过程}, label={c0301-train-batch}}
\begin{lstlisting}
def train_batch(self, model, optimizer, x, n_way, k_shot, 
            q_query, loss_fn, inner_train_steps= 1, 
            inner_lr = 0.4, train = True):
    """
    Args:
    x is the input omniglot images for a meta_step, shape = [batch_size, n_way * (k_shot + q_query), 1, 28, 28]
    n_way: 每個分類的 task 要有幾個 class
    k_shot: 每個類別在 training 的時候會有多少張照片
    q_query: 在 testing 時，每個類別會用多少張照片 update
    """
    criterion = loss_fn
    task_loss = []
    task_acc = []
    for meta_batch in x:
        train_set = meta_batch[:n_way*k_shot]
        val_set = meta_batch[n_way*k_shot:]
        fast_weights = OrderedDict(model.named_parameters())
        for inner_step in range(inner_train_steps):
            train_label = self.create_label(n_way, k_shot).to(self.device)
            logits = model.functional_forward(train_set, fast_weights)
            loss = criterion(logits, train_label)
            grads = torch.autograd.grad(loss, fast_weights.values(), create_graph = True)
            fast_weights = OrderedDict((name, param - inner_lr * grad)
                              for ((name, param), grad) in zip(fast_weights.items(), grads))
        val_label = self.create_label(n_way, q_query).to(self.device)
        logits = model.functional_forward(val_set, fast_weights)
        loss = criterion(logits, val_label)
        task_loss.append(loss)
        acc = np.asarray([torch.argmax(logits, -1).cpu().numpy() == val_label.cpu().numpy()]).mean()
        task_acc.append(acc)
    model.train()
    optimizer.zero_grad()
    meta_batch_loss = torch.stack(task_loss).mean()
    if train:
        meta_batch_loss.backward()
        optimizer.step()
    task_acc = np.mean(task_acc)
    return meta_batch_loss, task_acc
\end{lstlisting}
代码解读如下所示：
\begin{itemize}
\item 第14行：每个meta\_batch的形状为[10, 1, 28, 28]；
\item 第15行：前n\_way*k\_shot个样本为train\_set（或者叫support set）；
\item 第16行：其余样本为验证数据集（或者叫query set）；
\item 第17行：拷贝一份模型参数，在本例中只有一个任务，所以只拷贝了一份，如果是多个任务，需要拷贝多份；
\item 第18行：内循环通常为一次，也可能是多次（以下是对于一个任务的处理流程）:
\item 第19行：生成正确答案，对于很多任务，正确答案在数据集中已经定义好了；
\item 第20行：调用模型前向传播过程，产生网络输出，代码见表\ref{c0301-ogml-model}：
	\begin{itemize}
	\item 第41$\sim$45行：循环处理四个卷积块，调用ConvBlockFunction函数，从命名参数中：conv1.0.weight和conv1.0.bias为卷操作参数，；
	\end{itemize}
\item 第行：；
\item 第行：；
\item 第行：；
\end{itemize}

















\section{当前位置}





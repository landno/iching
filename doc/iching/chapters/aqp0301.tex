\newpage
\maketitle
\begin{center}
\Large \textbf{第301章 MAML算法} \quad 
\end{center}
\begin{abstract}
在本章中，我们将讲解MAML算法的基本原理，并且以Omniglot数据集为例，讲解一个5-way 1-shot的算法实现，并且复现论文中的结果。
\end{abstract}
\section{MAML算法}
在元学习中，最著名的算法当属MAML算法，在本章中，我们将讲解MAML算法数学原理和PyTorch实现技术。
\subsection{Omniglot数据集}
Omniglot数据集的目录结构如下所示：
\begin{figure}[H]
	\caption{Omniglot数据集目录结构}
	\label{f000116}
	\centering
	\includegraphics[width=15cm]{images/f000116}
\end{figure}
数据集类代码如下所示：
\lstset{language=PYTHON, caption={Omniglot数据集类}, label={c0301-omniglot-ds}}
\begin{lstlisting}
class OmniglotDs(Dataset):
    def __init__(self, data_dir, k_way, q_query):
        self.file_list = [f for f in glob.glob(data_dir + 
                    '**/character*', recursive=True)]
        self.transform = transforms.Compose([transforms.ToTensor()])
        self.n = k_way + q_query

    def __getitem__(self, idx):
        sample = np.arange(20)
        np.random.shuffle(sample)
        img_path = self.file_list[idx]
        img_list = [f for f in glob.glob(img_path + "**/*.png", 
                    recursive=True)]
        img_list.sort()
        imgs = [self.transform(Image.open(img_file))
                     for img_file in img_list]
        imgs = torch.stack(imgs)[sample[:self.n]]
        return imgs

    def __len__(self):
        return len(self.file_list)   
\end{lstlisting}
代码解读如下所示：
\begin{itemize}
\item 第3、4行：取出图片文件最后一层文件目录的列表，如下所示：
\lstset{language=BASH, caption={self.file\_list内容}, label={c0301-omniglot-ds-file-list}}
\begin{lstlisting}
./data/Omniglot/images_background\Alphabet_of_the_Magi.0\character01
./data/Omniglot/images_background\Alphabet_of_the_Magi.0\character02
./data/Omniglot/images_background\Alphabet_of_the_Magi.0\character03
./data/Omniglot/images_background\Alphabet_of_the_Magi.0\character04
./data/Omniglot/images_background\Alphabet_of_the_Magi.0\character05
./data/Omniglot/images_background\Alphabet_of_the_Magi.0\character06
./data/Omniglot/images_background\Alphabet_of_the_Magi.0\character07
./data/Omniglot/images_background\Alphabet_of_the_Magi.0\character08
\end{lstlisting}
\item 第6行：每个批次中Support Set的大小为k\_way，Query Set的大小为q\_query，该批次的总大小为self.n；
\item 第11行：取到某一个包含图片文件的目录；
\item 第12$\sim$14行取出该目录下所有图片文件列表，每个目录下有20个文件；
\item 第15、16行：类型为list[tensor]，将目录下每个图片内容读出来转为tensor，形状为$[1, 28, 28]$，即黑白图片（通道数），分辨率为$28 \times 28$，20个图片文件形成的tensor组成一个list；
\item 第17行：因为在第9行，生成[0,1,...,19]的列表，然后将其随机进行排序，运行结果如下所示：
\lstset{language=BASH, caption={运行原理}, label={c0301-omniglot-ds-get-item}}
\begin{lstlisting}
sample: [ 7  0 18 16 15 17  9 12 11  1  5 14  4  6  8  3  2 10 19 13];
sample[:self.n]: [7 0];
[sample[:self.n]]: [array([7, 0])];
imgs: torch.Size([2, 1, 28, 28]);
\end{lstlisting}
由此可见其形成的是一个训练中用到的迷你批次。
\end{itemize}
下面我们来看模型类OgmlModel，如下所示：
\lstset{language=PYTHON, caption={OgmlModel类}, label={c0301-ogml-model}}
\begin{lstlisting}
def ConvBlock(in_ch, out_ch):
    return nn.Sequential(nn.Conv2d(in_ch, out_ch, 3, padding = 1),
                        nn.BatchNorm2d(out_ch),
                        nn.ReLU(),
                        nn.MaxPool2d(kernel_size = 2, stride = 2)) # 原作者在 paper 裡是說她在 omniglot 用的是 strided convolution
                                                                    # 不過這裡我改成 max pool (mini imagenet 才是 max pool)
                                                                    # 這並不是你們在 report 第三題要找的 tip

def ConvBlockFunction(x, w, b, w_bn, b_bn):
    x = F.conv2d(x, w, b, padding = 1)
    x = F.batch_norm(x, running_mean = None, running_var = None, weight = w_bn, bias = b_bn, training = True)
    x = F.relu(x)
    x = F.max_pool2d(x, kernel_size = 2, stride = 2)
    return x

class OgmlModel(nn.Module):
    def __init__(self, in_ch, k_way):
        super(OgmlModel, self).__init__()
        self.conv1 = ConvBlock(in_ch, 64)
        self.conv2 = ConvBlock(64, 64)
        self.conv3 = ConvBlock(64, 64)
        self.conv4 = ConvBlock(64, 64)
        self.logits = nn.Linear(64, k_way)
    
    def forward(self, x):
        x = self.conv1(x)
        x = self.conv2(x)
        x = self.conv3(x)
        x = self.conv4(x)
        x = nn.Flatten(x)
        x = self.logits(x)
        return x

    def functional_forward(self, x, params):
        '''
        Arguments:
        x: input images [batch, 1, 28, 28]
        params: 模型的參數，也就是 convolution 的 weight 跟 bias，以及 batchnormalization 的  weight 跟 bias
                這是一個 OrderedDict
        '''
        for block in [1, 2, 3, 4]:
            x = ConvBlockFunction(x, params[f'conv{block}.0.weight'], params[f'conv{block}.0.bias'],
                                params.get(f'conv{block}.1.weight'), params.get(f'conv{block}.1.bias'))
        x = x.view(x.shape[0], -1)
        x = F.linear(x, params['logits.weight'] , params['logits.bias'])
        return x
\end{lstlisting}
OgmlModel有两种工作模式，一种是正常的forward模式，由构造函数、ConvBolock函数和forward方法定义，另一种是functional\_forward模式，由ConvBlockFunction和functional\_forward方法组成，在程序中我们使用的是functional\_forward方法，因此我们只讲解这种工作模式。
\begin{itemize}
\item 第行：；
\item 第行：；
\item 第行：；
\item 第行：；
\end{itemize}




